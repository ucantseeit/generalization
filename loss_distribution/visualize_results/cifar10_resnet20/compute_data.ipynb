{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade34230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hqdeng7/lijuyang/generalization/loss_distribution', '/home/hqdeng7/.conda/envs/ljy/lib/python311.zip', '/home/hqdeng7/.conda/envs/ljy/lib/python3.11', '/home/hqdeng7/.conda/envs/ljy/lib/python3.11/lib-dynload', '', '/home/hqdeng7/.conda/envs/ljy/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "project_root_dir = os.path.abspath(os.path.join(current_notebook_dir, '../../'))\n",
    "\n",
    "# 将这个父目录添加到sys.path的最前面\n",
    "if project_root_dir not in sys.path:\n",
    "    sys.path.insert(0, project_root_dir)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e48427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_script.visual_utils import load_cifar10_data, \\\n",
    "\tget_sorted_model_paths, evaluate_model_performance, load_model_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e413671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR-10 数据集到 ../../pytorch_script/data/cifar10...\n",
      "CIFAR-10 数据集加载成功！\n",
      "\n",
      "开始逐个评估模型...\n",
      "\n",
      "正在评估模型：../../model_training_results/cifar10_resnet20/model_0.pth (纪元: 0)\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "  training acc = 48.33%\n",
      "  training loss = 1.47464\n",
      "  test acc = 47.84%\n",
      "  test loss = 1.47464\n",
      "\n",
      "正在评估模型：../../model_training_results/cifar10_resnet20/model_5.pth (纪元: 5)\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "  training acc = 73.19%\n",
      "  training loss = 0.80459\n",
      "  test acc = 72.31%\n",
      "  test loss = 0.80459\n",
      "\n",
      "正在评估模型：../../model_training_results/cifar10_resnet20/model_10.pth (纪元: 10)\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "  training acc = 58.85%\n",
      "  training loss = 1.80034\n",
      "  test acc = 57.67%\n",
      "  test loss = 1.80034\n",
      "\n",
      "正在评估模型：../../model_training_results/cifar10_resnet20/model_20.pth (纪元: 20)\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "  training acc = 72.31%\n",
      "  training loss = 0.92275\n",
      "  test acc = 70.61%\n",
      "  test loss = 0.92275\n",
      "\n",
      "正在评估模型：../../model_training_results/cifar10_resnet20/model_30.pth (纪元: 30)\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "  training acc = 70.76%\n",
      "  training loss = 0.91712\n",
      "  test acc = 68.52%\n",
      "  test loss = 0.91712\n",
      "\n",
      "正在评估模型：../../model_training_results/cifar10_resnet20/model_40.pth (纪元: 40)\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "  training acc = 79.78%\n",
      "  training loss = 0.60316\n",
      "  test acc = 76.84%\n",
      "  test loss = 0.60316\n",
      "\n",
      "正在评估模型：../../model_training_results/cifar10_resnet20/model_50.pth (纪元: 50)\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 评估训练集性能\u001b[39;00m\n\u001b[32m     40\u001b[39m model = load_model_state_dict(\u001b[33m'\u001b[39m\u001b[33mcifar10\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mresnet20\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m10\u001b[39m, model_path, device)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m train_acc, train_loss = \u001b[43mevaluate_model_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_acc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     43\u001b[39m \ttrain_accuracies.append(train_acc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lijuyang/generalization/loss_distribution/pytorch_script/visual_utils.py:77\u001b[39m, in \u001b[36mevaluate_model_performance\u001b[39m\u001b[34m(model, data_loader, device)\u001b[39m\n\u001b[32m     75\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): \u001b[38;5;66;03m# 在评估时不计算梯度\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/multiprocessing/connection.py:948\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    945\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m    950\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ljy/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28mself\u001b[39m._selector.poll(timeout)\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "config = {\n",
    "\t'model_dir': '../../model_training_results/cifar10_resnet20',  # 模型存储的文件夹路径\n",
    "\t'model_prefix': 'model_',        # 模型文件名的前缀\n",
    "\t'model_extension': '.pth',                         # 保存模型的文件扩展名\n",
    "\t'cifar10_data_path': '../../pytorch_script/data/cifar10',           # CIFAR-10 数据集存储路径\n",
    "\t'batch_size': 64,                                 # DataLoader 的批次大小\n",
    "\t'num_workers': 2                                  # DataLoader 的工作进程数\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 加载数据\n",
    "train_loader, test_loader = load_cifar10_data(\n",
    "\tconfig['cifar10_data_path'], config['batch_size'], config['num_workers']\n",
    ")\n",
    "if train_loader is None or test_loader is None:\n",
    "\tprint(\"数据加载失败，程序终止。\")\n",
    "\n",
    "# 获取排序后的模型文件路径\n",
    "sorted_model_paths = get_sorted_model_paths(\n",
    "\tconfig['model_dir'], config['model_prefix'], config['model_extension']\n",
    ")\n",
    "\n",
    "if not sorted_model_paths:\n",
    "\tprint(\"没有找到符合条件或能成功加载的模型文件。请检查 MODEL_DIR, MODEL_PREFIX 和 MODEL_EXTENSION 设置。\")\n",
    "\n",
    "epochs = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"\\n开始逐个评估模型...\")\n",
    "for epoch, model_path in sorted_model_paths:\n",
    "\tprint(f\"\\n正在评估模型：{model_path} (纪元: {epoch})\")\n",
    "\t\n",
    "\t# 评估训练集性能\n",
    "\tmodel = load_model_state_dict('cifar10', 'resnet20', 10, model_path, device)\n",
    "\ttrain_acc, train_loss = evaluate_model_performance(model, train_loader, device)\n",
    "\tif train_acc is not None:\n",
    "\t\ttrain_accuracies.append(train_acc)\n",
    "\t\ttrain_losses.append(train_loss)\n",
    "\t\tprint(f\"  training acc = {train_acc:.2f}%\")\n",
    "\t\tprint(f\"  training loss = {train_loss:.5f}\")\n",
    "\telse:\n",
    "\t\tprint(\"  训练集评估失败，跳过。\")\n",
    "\t\tcontinue # 如果训练集评估失败，则整个模型跳过\n",
    "\n",
    "\t# 评估测试集性能\n",
    "\ttest_acc, test_loss = evaluate_model_performance(model, test_loader, device)\n",
    "\tif test_acc is not None:\n",
    "\t\ttest_accuracies.append(test_acc)\n",
    "\t\ttest_losses.append(test_loss)\n",
    "\t\tprint(f\"  test acc = {test_acc:.2f}%\")\n",
    "\t\tprint(f\"  test loss = {test_loss:.5f}\")\n",
    "\telse:\n",
    "\t\tprint(\"  测试集评估失败，跳过。\")\n",
    "\t\tcontinue # 如果测试集评估失败，则整个模型跳过\n",
    "\n",
    "\tepochs.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b8062",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_accuracies.npy', np.array(train_accuracies))\n",
    "np.save('test_accuracies.npy', np.array(test_accuracies))\n",
    "np.save('train_losses.npy', np.array(train_losses))\n",
    "np.save('test_losses.npy', np.array(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "221f5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_losses(model, data_loader):\n",
    "  losses = []\n",
    "  model.eval()\n",
    "  for images, labels in data_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, labels, reduction='none')\n",
    "    losses.extend(loss.tolist())\n",
    "  return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2cfcc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "10\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "20\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "30\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "40\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "50\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "60\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "70\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "80\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "90\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "100\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "110\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "120\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "130\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "140\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "150\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "160\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "170\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "180\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "190\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "200\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "210\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "220\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "230\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "240\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "250\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "260\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "270\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "280\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "290\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "300\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "310\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "320\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "330\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "340\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "350\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "360\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "370\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "380\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "390\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "400\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "410\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "420\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "430\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "440\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "450\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "460\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "470\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "480\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "490\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n",
      "500\n",
      "  从字典中提取模型状态字典...\n",
      "提取成功\n"
     ]
    }
   ],
   "source": [
    "all_model_train_losses = {}\n",
    "all_model_test_losses = {}\n",
    "\n",
    "for epoch, model_path in sorted_model_paths:\n",
    "\tprint(epoch)\n",
    "\tmodel = load_model_state_dict('cifar10', 'resnet20', 10, model_path, device)\n",
    "\tmodel.to(device)\n",
    "\tall_model_train_losses[epoch] = model_losses(model, train_loader)\n",
    "\tall_model_test_losses[epoch] = model_losses(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5459edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path_pickle = \"all_model_train_losses.pickle\"\n",
    "with open(file_path_pickle, 'wb') as f: # 注意 'wb' 表示写入二进制\n",
    "    pickle.dump(all_model_train_losses, f)\n",
    "\n",
    "file_path_pickle = \"all_model_test_losses.pickle\"\n",
    "with open(file_path_pickle, 'wb') as f: # 注意 'wb' 表示写入二进制\n",
    "    pickle.dump(all_model_test_losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "148570df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('epochs.npy', epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e441cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ljy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
